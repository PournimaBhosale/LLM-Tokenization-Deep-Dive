# LLM Tokenization Deep Dive

This repository presents a structured and practical understanding of
**tokenization in Large Language Models (LLMs)**.

Tokenization is a critical yet often overlooked component of LLMs.
It directly impacts model performance, inference cost, context limits,
and how language is internally represented.

This repository documents tokenization concepts clearly, without
over-simplification, and complements theory with small, focused examples.

---

## What You Will Learn

- Why tokenization exists and how it works
- Differences between word, character, and subword tokenization
- Subword algorithms used in modern LLMs
- The complete tokenization pipeline
- How GPT, BERT, and LLaMA tokenize text
- Practical edge cases (emojis, whitespace, multilingual input)

---

## Repository Structure

- `docs/` – Conceptual explanations
- `hands_on/` – Small, focused code examples
- `examples/` – Model-wise behavior comparisons
- `resources/` – References and further reading

---

## Intended Audience

- AI / ML students
- Engineers working with LLM-based systems
- Interview preparation
- Anyone who wants to understand how text becomes tokens
