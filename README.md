# LLM-Tokenization-Deep-Dive
# LLM Tokenization Deep Dive

This repository documents a structured and practical understanding of
tokenization in Large Language Models (LLMs).

It focuses on how raw text is transformed into tokens, how different
tokenization strategies work, and how modern models such as GPT, BERT,
and LLaMA handle text internally.

---

## Contents

- Tokenization fundamentals
- Word, character, and subword tokenization
- Subword algorithms: BPE, WordPiece, Unigram
- Tokenization pipeline in LLMs
- Model-specific tokenizers
- Practical edge cases

---

## Structure

- `docs/` – Conceptual explanations
- `hands_on/` – Practical code examples
- `examples/` – Behavior comparisons
- `resources/` – Further reading

---

## Purpose

This repository is intended for learning, revision, and interview
preparation, with emphasis on clarity and correctness.
