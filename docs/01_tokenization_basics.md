# Tokenization Basics

Tokenization is the process of converting raw text into smaller units
called tokens that can be processed by language models.

In LLMs, tokenization forms the foundation of how language is understood
and represented numerically.
