# Tokenizers in LLMs

- GPT: Byte-level BPE
- BERT: WordPiece
- LLaMA: SentencePiece (Unigram)

## üß© Tokenizers in Large Language Models (LLMs)

A tokenizer is the **bridge between human language and machine
understanding**. Large Language Models (LLMs) do not read text directly;
they operate on **tokens represented as numbers**. The tokenizer defines
how text is split, encoded, and reconstructed.

Choosing the right tokenizer directly affects model performance,
efficiency, and cost.

---

## What Is a Tokenizer?

A tokenizer is a system that:
1. Splits text into tokens
2. Converts tokens into numerical IDs
3. Handles special symbols and formatting
4. Reconstructs text during decoding

Without a tokenizer, an LLM cannot process text.

---

##  Core Components of a Tokenizer

Every tokenizer used in LLMs consists of the following components:

### 1Ô∏è‚É£ Vocabulary

- A fixed list of tokens known to the model
- Each token has a unique ID
- Vocabulary size impacts memory and efficiency

Example:
"hello" ‚Üí 15496
"world" ‚Üí 2159


---

### 2Ô∏è‚É£ Tokenization Algorithm

Defines **how text is split into tokens**.

Common algorithms:
- Byte Pair Encoding (BPE)
- WordPiece
- Unigram Language Model

This algorithm decides how words are broken into subwords.

---

### 3Ô∏è‚É£ Special Tokens

Special tokens guide model behavior.

Common examples:
- `[CLS]` ‚Äì Classification token (BERT)
- `[SEP]` ‚Äì Sentence separator
- `<BOS>` ‚Äì Beginning of sequence
- `<EOS>` ‚Äì End of sequence
- `<PAD>` ‚Äì Padding token

---

### 4Ô∏è‚É£ Encoder and Decoder

- **Encoder**: Converts text ‚Üí tokens ‚Üí IDs
- **Decoder**: Converts IDs ‚Üí tokens ‚Üí text

This ensures that the process is reversible.

---

## üîπ Tokenizers Used in Popular LLMs

Different models use different tokenization strategies.

---

### üî∏ GPT Family (GPT-2, GPT-3, GPT-4)

**Tokenizer Type:** Byte Pair Encoding (BPE)  
**Key Feature:** Byte-level tokenization

Example: "ChatGPT" ‚Üí ["Chat", "G", "PT"]


Why GPT uses BPE:
- Handles any Unicode input
- No unknown tokens
- Efficient for open-domain text

---

### üî∏ BERT

**Tokenizer Type:** WordPiece  
**Key Feature:** Subword tokens with `##` prefix

Example:"playing" ‚Üí ["play", "##ing"]

Why BERT uses WordPiece:
- Strong handling of rare words
- Optimized for bidirectional context

---

### üî∏ LLaMA

**Tokenizer Type:** SentencePiece (Unigram)  
**Key Feature:** Language-agnostic tokenization

Example:"unbelievable" ‚Üí ["un", "believ", "able"]


Why LLaMA uses SentencePiece:
- Works without whitespace
- Suitable for multilingual models

---

## Byte-Level vs Word-Level Tokenizers

| Feature | Byte-Level | Word-Level |
|------|-----------|-----------|
| Unknown words | Never | Frequent |
| Vocabulary size | Moderate | Large |
| Multilingual support | Strong | Weak |
| Used in LLMs | ‚úÖ Yes | ‚ùå No |

Modern LLMs strongly prefer **byte or subword tokenizers**.

---

## Common Tokenization Pitfalls

- Different tokenizers produce different token counts
- Tokenization mismatches break fine-tuning
- Special tokens must match model training

> Always use the tokenizer shipped with the model.

---

## üéØ Key Takeaways

- Tokenizers define how LLMs understand language
- Subword tokenization is the industry standard
- Tokenizer choice impacts cost, accuracy, and latency
- Model and tokenizer must always stay aligned
