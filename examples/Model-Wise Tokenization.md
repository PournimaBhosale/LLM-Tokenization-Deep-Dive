## Model-Wise Tokenization Comparison

Tokenization is not universal â€” different LLMs **split the same text differently**.  
Understanding model-wise behavior is critical for:
- Predicting token counts
- Managing context windows
- Writing efficient prompts

Let's explore GPT-2, BERT, and LLaMA using a simple sentence.

---

## ðŸ”¹ Example Sentence

"Tokenization helps LLMs understand language."

---

## ðŸŸ¢ 1. GPT-2 Tokenization

**Tokenizer:** Byte Pair Encoding (BPE)  
**Key feature:** Byte-level â†’ handles any Unicode input  
**Behavior:** Breaks words into subword units efficiently

| Token | Token ID |
|-------|----------|
| Token | 13301    |
| ization | 7451    |
| helps | 1623     |
| LL | 287      |
| Ms | 1327     |
| understand | 1037 |
| language | 1859   |
| .     | 13       |

**Token count:** 8 tokens  

> GPT-2 captures subwords, making it efficient even for new words.

---

## ðŸŸ¡ 2. BERT Tokenization

**Tokenizer:** WordPiece  
**Key feature:** Subword tokenization with `##` prefix for suffixes

| Token | Token ID |
|-------|----------|
| token | 19204    |
| ##ization | 14567 |
| helps | 2391     |
| ll | 103      |
| ##ms | 215     |
| understand | 4023 |
| language | 2873  |
| .     | 1012     |

**Token count:** 8 tokens  

> BERT uses `##` to mark subword continuation â€” great for rare words.

---

## ðŸ”µ 3. LLaMA Tokenization

**Tokenizer:** SentencePiece (Unigram)  
**Key feature:** Language-agnostic, handles whitespace-free languages

| Token | Token ID |
|-------|----------|
| Token | 1562     |
| ization | 2984    |
| helps | 842      |
| LL | 74        |
| Ms | 162      |
| understand | 998 |
| language | 1876 |
| .     | 13       |

**Token count:** 8 tokens  

> LLaMA works well across multiple languages and scripts.

---

## âš¡ Key Insights

1. **All models have same token count** in this simple example, but **tokens differ internally**.  
2. Subword tokenization allows **handling unseen words**.  
3. GPT-2 is **byte-level**, BERT is **WordPiece**, LLaMA is **Unigram SentencePiece**.  
4. Different tokenization strategies affect **cost, model input, and context window usage**.




