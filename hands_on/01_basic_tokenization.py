# 01_basic_tokenization.py

text = "Hello world! Tokenization is fun."

# Word Tokenization
word_tokens = text.split()
print("Word Tokens:", word_tokens)

# Character Tokenization
char_tokens = list(text)
print("Character Tokens:", char_tokens)
